\begin{cabstract}
联邦学习是一种注重隐私的分布式机器学习范式，它将参与方的数据保留在本地，联合各方的本地模型参数，聚合生成全局模型参数，进而完成联合训练，在解决数据孤岛问题上展现了巨大的潜力。但是相关研究表明，参与方上传的模型参数携带有原始数据的特征，仍然有泄露原始隐私数据的风险。同时，一些具体场景给经典的模型参数聚合算法（FedAvg）带来了新的挑战，比如众多物联网设备联合训练时潜在的拜占庭节点（恶意节点）干扰，以及医疗金融等领域联合训练时受到的异质分布数据（非独立同分布数据）影响。
这揭示了模型参数的隐私性和其在具体场景下的可用性之间的矛盾。一方面，对模型参数的隐私保护需求需要对参数进行加密或者扰动，以预防潜在的隐私泄露风险；另一方面，一些具体的联邦学习场景需要引入更加复杂的计算保证模型参数的鲁棒聚合，这进一步提升了对模型参数的可用性需求。如何权衡模型参数的隐私性和日益增长的模型参数可用性需求，是一个非常有挑战性且有实际应用价值的难题。
%经典的联邦学习聚合算法（FedAvg）性能急剧下降。
%同时一些具体的联邦学习场景也给经典的联邦学习聚合算法带来了新的挑战。
%同时更加复杂的应用场景也给联邦学习带来了新的挑战。一方面，对模型参数的隐私保护需求需要对参数进行扰动或者加密，以预防潜在的隐私泄露风险；另一方面，复杂场景下的联邦学习也引入了更加复杂的模型参数聚合算法，如何在保护模型参数隐私的同时，实现高效、鲁棒的联邦学习方案，是联邦学习走向更复杂应用场景必须要解决的难题。

本文的研究内容立足于解决联邦学习中模型参数的隐私泄露问题，具体分别考虑两种场景对模型参数安全高效聚合带来的挑战：$(\rm \romannumeral1)$ 拜占庭节点的干扰；$(\rm \romannumeral2)$异质分布数据的影响。
论文主要工作与核心贡献如下：

（1）针对安全性较弱的物联网场景下拜占庭节点给联邦学习造成的安全威胁，同时兼顾对模型参数的隐私保护，本文提出了一种高效的面向拜占庭容错的模型参数安全聚合技术。具体来说，首先提出了一种密文计算友好的拜占庭鲁棒聚合算法，然后利用多方同态加密技术设计隐私计算模块，在密文上实现了对拜占庭节点的鲁棒聚合。理论分析结合实验结果表明，方案在密文上完成了对恶意模型参数的过滤，充分利用良性用户的模型参数训练全局模型，实现了较高的推理准确率，同时轻量的拜占庭鲁棒聚合算法也保证了密文计算上的高效性。此外，本文提出的密文计算友好的拜占庭容错聚合算法，理论上支持采用不同的隐私保护技术来实现密文上的鲁棒聚合，具有较好的扩展性。

（2）针对医疗金融等领域中常见的异质分布数据给联邦学习带来的挑战，同时兼顾对模型参数的隐私保护，本文提出了一种高效的面向异质分布数据的模型参数安全聚合技术。
%提升异质分布数据训练准确率的隐私保护联邦学习方案。
具体而言，基于扩展性较好的引入聚类方法的联邦学习方案，利用秘密共享、伪随机生成以及Diffie-Hellman密钥协商等密码学技术，高效实现了基于聚类方法的安全聚合，同时保证了对模型参数的完全隐私保护。理论分析和实验验证表明，方案在对模型参数提供强隐私保护的同时，大幅提升了异质分布数据联合训练的性能。此外，本文提出的方案具有较好的扩展性，理论上可以结合其它提升异质分布数据训练性能的方案，在保护模型参数隐私的前提下，进一步提升全局模型的推理准确率。

%\begin{compactenum}
%	\item 针对联邦学习中的隐私威胁和拜占庭节点威胁，本文提出了一种拜占庭容错的隐私保护联邦学习方案。具体来说，本文首先提出了一种密文计算友好的拜占庭鲁棒聚合方案，再利用多方同态加密作为隐私计算模块，在密文上实现了对拜占庭节点的鲁棒聚合。理论分析结合实验结果表明，方案实现了对中间模型参数的隐私保护、对拜占庭节点的鲁棒性以及聚合计算的高效性。
%	\item 为了应对联邦学习中异质分布数据带来的挑战，同时兼顾对中间模型参数的隐私保护，本文提出了一种高效的提升异质分布数据训练准确率的隐私保护联邦学习方案。具体而言，本文基于扩展性较好的聚类异质联邦学习方案，利用秘密共享、伪随机生成以及密钥协商等密码技术，高效实现了对中间模型参数的完全隐私保护。理论分析和实验验证表明，方案在对中间模型参数提供强隐私保护的同时，大幅提升了异质分布数据联合训练的准确率。
%\end{compactenum}
\end{cabstract}
\ckeywords{联邦学习；隐私保护；拜占庭鲁棒；异质分布数据；多方同态加密；安全多方计算}

\begin{eabstract}
%Federated learning (FL), a privacy-preserving distributed machine learning paradigm that keeps private data local and performs joint training by exchanging model parameters, showing great potential in solving the problem of data silos. However, some researches show that model parameters still have the risk of leaking private data, while more complex application scenarios also bring new challenges to FL. On the one hand, the privacy protection of model parameters requires perturbation or encryption of parameters to prevent potential privacy leakage risk; on the other hand, FL in complex scenarios also introduces more complex model parameter aggregation schemes, and how to achieve an efficient and robust FL scheme while protecting the privacy of model parameters is necessary for FL to be applied in complex application scenarios.

Federated learning (FL) is a privacy-conscious distributed machine learning paradigm, which keeps the data of the participants locally and aggregates the local model parameters of each party to generate global model parameters for joint training, showing great potential in solving the problem of data islands. However, some researches show that the model parameters uploaded by the participants carry the characteristics of the original data, and there is still a risk of leaking the original private data. Meanwhile, some specific scenarios bring new challenges to the classical model parameter aggregation algorithm (FedAvg), such as the interference of Byzantine nodes (malicious nodes) that may occur when many IoT devices are jointly trained, and the influence of heterogeneous distribution data (non-independent identically distributed data) when jointly trained in fields such as healthcare and finance.
This reveals conflicts between the privacy of model parameters and their usability in specific scenarios. On the one hand, the privacy protection of model parameters requires encryption or perturbation of parameters to prevent potential privacy leakage risks; on the other hand, some specific FL scenarios require complex computations to ensure robust aggregation of model parameters, which further enhances the usability of model parameters. The trade-off between the privacy of model parameters and the increasing model parameter usability is a practical challenge.

This paper is devoted to privacy leakage threat in FL, and specifically considers the challenges posed by two specific scenarios for secure and efficient aggregation of model parameters: $(\rm \romannumeral1)$ interference from Byzantine nodes; and $(\rm \romannumeral2)$ the impact of heterogeneous distribution data. Specifically, the main work and core contributions of this paper are as follows:

(1) To address the secure threats caused by Byzantine nodes in IoT FL, while considering the privacy protection of model parameters, this paper proposes an efficient Byzantine fault-tolerant secure aggregation technique for model parameters.
%(1) To address the privacy threat and Byzantine threat in FL, this paper proposes an efficient privacy-preserving Byzantine-robust FL scheme.
In particular, this paper first proposes a ciphertext computationally friendly Byzantine-robust aggregation scheme, and then implements robust aggregation of Byzantine users over ciphertexts using Multiparty Homomorphic Encryption (MHE) as the privacy building block. The theoretical analysis and experimental results show that the scheme accomplishes the filtering of malicious parameters over ciphertexts, makes full use of the parameters of benign users to train the global model, and thus achieves high inference accuracy. Meanwhile, our lightweight Byzantine-robust aggregation scheme ensures high efficiency over ciphertext computation. In addition, the Byzantine-robust aggregation scheme proposed in this paper is theoretically scalable by using different privacy-preserving techniques to achieve robust aggregation over ciphertexts.

%(2) This paper proposes an efficient secure aggregation technique for model parameters for heterogeneous distributed data to address the challenges posed by federated learning in areas such as healthcare and finance, while taking into account the privacy protection of model parameters.
(2) To address the challenges posed by heterogeneous distribution data in FL of areas such as healthcare and finance, while protecting the privacy of model parameters, this paper proposes an efficient secure aggregation technique for model parameters for heterogeneous distribution data.
Specifically, based on a scalable FL scheme that introduces clustering methods, this paper uses cryptographic techniques such as Secret Sharing (SS), Pseudo-Random Generation (PRG), and Diffie-Hellman key agreement protocol to efficiently achieve secure aggregation based on clustering methods while ensuring full privacy protection of model parameters.
%Specifically, based on the well-expanded clustered heterogeneous FL scheme, this paper efficiently achieves full privacy protection of the model parameters by using cryptographic techniques such as Secret Sharing (SS), pseudo-random generation (PRG), and Diffie-Hellman key agreement protocol.
Theoretical analysis and experimental validation show that the scheme substantially improves the accuracy of joint training of heterogeneous distributed data while providing strong privacy protection for the model parameters.
%In addition, the scheme proposed has good scalability and can theoretically be combined with other schemes that improve the training performance of heterogeneous distributed data to confer privacy protection on model parameters and further improve the inference accuracy of the global model.
In addition, the proposed scheme exhibits excellent scalability, which can be combined with other schemes designed for heterogeneous distribution data to further improve the performance of FL while preserving the privacy of model parameters.
%endow the privacy-preserving ability of model parameters, and thus further improve the inference accuracy of the global model.

\end{eabstract}
\ekeywords{Federated Learning, Privacy-preserving, Byzantine-robust, Heterogeneous distribution data, Multiparty Homomorphic Encryption, Secure Multi-Party Computation}

